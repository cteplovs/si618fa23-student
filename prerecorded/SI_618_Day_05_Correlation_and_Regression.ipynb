{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SI 618: Correlation and Regression\n",
    "\n",
    "Version 2021.02.16.1.CT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Correlation does not imply causation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/umsi-data-science/si370/raw/master/resources/piratesvstemp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "http://mpastell.com/2013/04/19/python_regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wine quality redux\n",
    "![](https://github.com/umsi-data-science/si370/raw/master/resources/vinho.png)\n",
    "https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009/home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Warnings usually just cause us unnessary stress. The next code block silences warnings. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "wine = pd.read_csv('https://raw.githubusercontent.com/umsi-data-science/si370/master/data/winequality-red.csv')\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Remember creating a pairplot for the wine dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(wine,vars = ['fixed acidity','volatile acidity','pH','residual sugar'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also look at the correlation between those variables.  From [Wikipedia](https://en.wikipedia.org/wiki/Correlation_and_dependence):\n",
    "> In the broadest sense correlation is any statistical association, though in common usage it most often refers to how close two variables are to having a linear relationship with each other...\n",
    "> Correlations are useful because they can indicate a predictive relationship that can be exploited in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Correlation coefficient can range from -1 to 1 and represent the degree to which and how\n",
    "two variables are related.  Here's a visualization of a range of correlation coefficients:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/umsi-data-science/si370/raw/master/resources/correlation1-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# to calculate all possible correlation coefficients in a dataframe\n",
    "wine.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A heatmap is a really useful visualization technique for large correlation matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(wine.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also use a different type of color palette to highlight the differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(wine.corr(),cmap=sns.color_palette(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall we used JointGrid to create a regplot and histplot on the same plot.  We can take a \n",
    "closer look at that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "wine.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "g = sns.JointGrid(data=wine,x='density',y='pH')\n",
    "g = g.plot(sns.regplot, sns.histplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6FyOXTsgU2ui",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ordinary Least Squares (OLS) Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can get a lot more detail about the regression model by using statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1JuuPtlTU2ui",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ATYDr80MU2uk",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "statsmodels uses R-Style formulas: y ~ x1 + x2 + x3 + ...\n",
    "\n",
    "1. y represents the outcome/dependent variable\n",
    "2. x1, x2, x3, etc represent explanatory/independent variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model0 = smf.ols(\" pH ~ density\", data=wine).fit()\n",
    "model0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the above example, the $r^2$ value is 0.117, which means the model explains about 11.7% of the variance (which isn't great).  However, the model is statistically significant (the p-value, called \"Prob (F-statistic)\" in the output, is less than 0.05).\n",
    "The estimates for the parameters of the model are also statistically significant, which means we can use the equation:\n",
    "\n",
    "pH = -27.9515 * density + 31.1716\n",
    "\n",
    "to predict the value of pH given a value for density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What happens when the series name has a space (or other special character) in it?  Use the 'Q' function to \"Quote\" the name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wNqlgMXHU2uk",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model1 = smf.ols(\" Q('fixed acidity') ~ density\", data=wine).fit()\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\"Outlier detection\" refers to the identification of points that are extreme in value.  They may or may not influence the regression analysis.\n",
    "\n",
    "Outlier: Large residual (i.e. big differences on the vertical dimension)\n",
    "\n",
    "Leverage: Extreme value of predictor variable (i.e. big differences on the horizontal dimension)\n",
    "\n",
    "Influence: Removing observation substantially changes estimate of coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider the following cases, drawn from \n",
    "\n",
    "http://songhuiming.github.io/pages/2016/11/27/linear-regression-in-python-outliers-leverage-detect/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the first case, there are no notable points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0) # seed the random number generator so we always get the same results\n",
    "x1 = np.random.normal(20, 3, 20) # create a random normal distribution of 20 points, \n",
    "                                 # centered on 20, standard deviation of 20\n",
    "y0 = 5 + 0.5 * x1 # create a corresponding y variable that is exactly 5 + 0.5 * x\n",
    "y1 = 5 + 0.5 * x1 + np.random.normal(0, 1, 20) # create a corresponding y variable that is 5 + 0.5 * x \n",
    "                                               # plus some random noise drawn from a normal distribution\n",
    "demo = pd.DataFrame({'x1':x1, 'y0':y0, 'y1':y1})\n",
    "lm = sm.OLS(y1, sm.add_constant(x1)).fit()\n",
    "lm1 = smf.ols(\" y1 ~ x1\", data=demo).fit()\n",
    "print(\"The rsquared value is \" + str(lm.rsquared))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(np.sort(demo.x1), demo.y1[np.argsort(demo.x1)])\n",
    "plt.scatter(np.mean(demo.x1), np.mean(demo.y1), color = \"green\")\n",
    "plt.plot(np.sort(demo.x1), demo.y0[np.argsort(demo.x1)], label = \"actual\")\n",
    "plt.plot(np.sort(demo.x1), lm.predict()[np.argsort(demo.x1)], label = \"regression\")\n",
    "plt.title(\"Linear Regression plots with the regression line\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# equivalently, without using DataFrames\n",
    "plt.scatter(np.sort(x1), y1[np.argsort(x1)])\n",
    "plt.scatter(np.mean(x1), np.mean(y1), color = \"green\")\n",
    "plt.plot(np.sort(x1), y0[np.argsort(x1)], label = \"actual\")\n",
    "plt.plot(np.sort(x1), lm.predict()[np.argsort(x1)], label = \"regression\")\n",
    "plt.title(\"Linear Regression plots with the regression line\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "fig = sm.graphics.influence_plot(lm, alpha  = 0.00005, ax = ax, criterion=\"cooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "High leverage point, no outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x2 = np.r_[x1, 30]\n",
    "y2 = np.r_[y1, 20.8]\n",
    "y20 = np.r_[y0, 20]\n",
    "\n",
    "lm2 = sm.OLS(y2, sm.add_constant(x2)).fit()\n",
    "print(\"The rsquared value is \" + str(lm2.rsquared))\n",
    "\n",
    "plt.scatter(np.sort(x2), y2[np.argsort(x2)])\n",
    "plt.scatter(30, 20.8, color = \"red\")\n",
    "plt.scatter(np.mean(x2), np.mean(y2), color = \"green\")\n",
    "plt.plot(np.sort(x2), y20[np.argsort(x2)], label = \"actual\")\n",
    "plt.plot(np.sort(x2), lm2.predict()[np.argsort(x2)], label = \"regression\")\n",
    "plt.legend()\n",
    "plt.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "fig = sm.graphics.influence_plot(lm2, ax= ax, criterion=\"cooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Outlier, no leverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x3 = np.r_[x1, 19]\n",
    "y3 = np.r_[y1, 20.8]\n",
    "y30 = np.r_[y0, 5 + .5 * 19]\n",
    "\n",
    "lm3 = sm.OLS(y3, sm.add_constant(x3)).fit()\n",
    "print(\"The rsquared value is \" + str(lm3.rsquared))\n",
    "\n",
    "plt.scatter(np.sort(x3), y3[np.argsort(x3)])\n",
    "plt.scatter(19, 20.8, color = \"red\")\n",
    "plt.scatter(np.mean(x3), np.mean(y3), color = \"green\")\n",
    "plt.plot(np.sort(x3), y30[np.argsort(x3)], label = \"actual\")\n",
    "plt.plot(np.sort(x3), lm3.predict()[np.argsort(x3)], label = \"regression\")\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "fig = sm.graphics.influence_plot(lm3, ax= ax, criterion=\"cooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Outlier and leverage point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x4 = np.r_[x1, 30]\n",
    "y4 = np.r_[y1, 10]\n",
    "y40 = np.r_[y0, 20]\n",
    "\n",
    "lm4 = sm.OLS(y4, sm.add_constant(x4)).fit()\n",
    "print(\"The rsquared value is \" + str(lm4.rsquared))\n",
    "\n",
    "plt.scatter(np.sort(x4), y4[np.argsort(x4)])\n",
    "plt.scatter(30, 10, color = \"red\")\n",
    "plt.scatter(np.mean(x4), np.mean(y4), color = \"green\")\n",
    "plt.plot(np.sort(x4), y40[np.argsort(x4)], label = \"actual line\")\n",
    "plt.plot(np.sort(x4), lm4.predict()[np.argsort(x4)], label = \"regression line\")\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "fig = sm.graphics.influence_plot(lm4, ax= ax, criterion=\"cooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's look at Anscombe's Quartet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "anscombe = sns.load_dataset(\"anscombe\")\n",
    "\n",
    "# Show the results of a linear regression within each dataset\n",
    "sns.lmplot(x=\"x\", y=\"y\", col=\"dataset\", hue=\"dataset\", data=anscombe,\n",
    "           col_wrap=2, ci=None, palette=\"muted\", height=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "anscombe.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "anscombe_1 = anscombe[anscombe.dataset == 'I']\n",
    "anscombe_2 = anscombe[anscombe.dataset == 'II']\n",
    "anscombe_3 = anscombe[anscombe.dataset == 'III']\n",
    "anscombe_4 = anscombe[anscombe.dataset == 'IV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "anscombe_1_lm = smf.ols('y ~ x',anscombe_1).fit()\n",
    "print(\"The rsquared value is \" + str(anscombe_1_lm.rsquared))\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "fig = sm.graphics.influence_plot(anscombe_1_lm, ax= ax, criterion=\"cooks\",alpha=0.05)\n",
    "                                     \n",
    "                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "anscombe_2_lm = smf.ols('y ~ x',anscombe_2).fit()\n",
    "print(\"The rsquared value is \" + str(anscombe_2_lm.rsquared))\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "fig = sm.graphics.influence_plot(anscombe_2_lm, ax= ax, criterion=\"cooks\",alpha=.05)\n",
    "                                     \n",
    "                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "anscombe_3_lm = smf.ols('y ~ x',anscombe_3).fit()\n",
    "print(\"The rsquared value is \" + str(anscombe_3_lm.rsquared))\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "fig = sm.graphics.influence_plot(anscombe_3_lm, ax= ax, criterion=\"cooks\",alpha=0.05)\n",
    "                                     \n",
    "                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "anscombe_4.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "anscombe_4_lm = smf.ols('y ~ x',anscombe_4).fit()\n",
    "print(\"The rsquared value is \" + str(anscombe_4_lm.rsquared))\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "fig = sm.graphics.influence_plot(anscombe_4_lm, ax= ax, criterion=\"cooks\",alpha=0.05)\n",
    "                                     \n",
    "                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "infl = anscombe_3_lm.get_influence()\n",
    "sm_fr = infl.summary_frame()\n",
    "sm_fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also extract the Cook's Distance values and their associated p-values from the influence object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c,p = infl.cooks_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "and create a DataFrame with the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict({'cooks_distance':c,'pvalue':p})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could sort the above in various, helpful ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
